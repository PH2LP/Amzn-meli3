#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
MEGA STRESS TEST - Smart Answer Engine v2.0

Test con 200 productos reales y preguntas VARIADAS:
- F√°ciles y dif√≠ciles
- Raras y rebuscadas (como clientes reales)
- Que requieran buscar info en JSONs
- Casos edge y normales
"""

import os
import sys
import json
import random
from pathlib import Path
from datetime import datetime
import openai
from dotenv import load_dotenv

sys.path.insert(0, 'scripts/tools')
from smart_answer_engine_v2 import answer_question_v2

load_dotenv()
openai.api_key = os.getenv("OPENAI_API_KEY")

# ============================================================================
# CONFIGURACI√ìN
# ============================================================================

CONFIG = {
    "max_products": 300,  # Test completo con 300 productos
    "questions_per_product": 1,  # 1 pregunta variada por producto
    "save_results": True,
    "output_dir": "test_results_stress"
}

# ============================================================================
# CARGA DE PRODUCTOS DESDE DB
# ============================================================================

def load_products_from_db(max_products=200):
    """
    Carga productos con mini_ml disponibles
    """
    print(f"\nüì¶ Cargando hasta {max_products} productos desde DB...")

    products_dir = Path("storage/logs/publish_ready")
    mini_ml_files = list(products_dir.glob("*_mini_ml.json"))

    print(f"  Encontrados: {len(mini_ml_files)} archivos mini_ml")

    # Mezclar aleatoriamente para variedad
    random.shuffle(mini_ml_files)

    # Limitar cantidad
    mini_ml_files = mini_ml_files[:max_products]

    products = []

    for file_path in mini_ml_files:
        asin = file_path.stem.replace("_mini_ml", "")

        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)

            products.append({
                "asin": asin,
                "title": data.get("title_ai", "")[:80],
                "brand": data.get("brand", ""),
                "category": data.get("category_name", ""),
                "price": data.get("price", 0),
                "data": data
            })
        except Exception as e:
            print(f"  ‚ö†Ô∏è  Error cargando {asin}: {e}")

    print(f"  ‚úÖ {len(products)} productos cargados exitosamente")
    return products


# ============================================================================
# GENERACI√ìN DE PREGUNTAS VARIADAS Y REALISTAS
# ============================================================================

def generate_realistic_questions(product_info):
    """
    Genera 1 pregunta VARIADA para un producto.
    La pregunta puede ser f√°cil, media o dif√≠cil aleatoriamente.
    """

    product_summary = f"""
PRODUCTO: {product_info['title']}
MARCA: {product_info['brand']}
CATEGOR√çA: {product_info['category']}
PRECIO: ${product_info['price']}
"""

    prompt = f"""Eres un CLIENTE REAL de MercadoLibre haciendo UNA pregunta sobre un producto.

{product_summary}

Genera EXACTAMENTE 1 pregunta REALISTA y VARIADA:

La pregunta puede ser de CUALQUIER nivel (var√≠a aleatoriamente):

üü¢ F√ÅCIL/SIMPLE (~40% de las veces):
- Info b√°sica que deber√≠a estar en la descripci√≥n
- Color, tama√±o, peso, marca, qu√© incluye
- Ejemplos: "¬øDe qu√© material es?", "¬øViene con pilas?", "¬øQu√© tama√±o tiene?"

üü° MEDIA (~40% de las veces):
- Compatibilidad, funcionamiento, caracter√≠sticas t√©cnicas
- Requiere buscar en JSON o razonar
- Ejemplos: "¬øFunciona con mi iPhone 12?", "¬øSe puede lavar?", "¬øCu√°nto dura la bater√≠a con uso normal?"

üî¥ DIF√çCIL/RARA/REBUSCADA (~20% de las veces):
- Comparaciones raras, casos de uso espec√≠ficos
- Preguntas rebuscadas o con m√∫ltiples condiciones
- Ejemplos reales:
  * "Si lo uso en un d√≠a lluvioso, ¬øse puede mojar sin que se arruine?"
  * "Mi abuela tiene 80 a√±os y no sabe de tecnolog√≠a, ¬øes f√°cil de usar para ella?"
  * "¬øEl rojo es rojo Ferrari o m√°s tirando a bord√≥?"
  * "Tengo manos grandes, ¬øme va a quedar chico?"
  * "Lo quiero regalar, ¬øviene en caja bonita o en bolsa nom√°s?"

IMPORTANTE:
- Preguntas en espa√±ol argentino/latinoamericano
- Que suenen NATURALES, como un cliente real preguntar√≠a
- La pregunta dif√≠cil debe ser REBUSCADA pero realista
- NO preguntes por otros productos (b√∫squeda)
- NO preguntes cosas de seguridad cr√≠tica (voltaje, garant√≠as) en TODAS

Responde SOLO este JSON:
{{
  "pregunta": "la pregunta aqu√≠",
  "dificultad": "facil|media|dificil",
  "tipo": "simple|compatibility|funcionamiento|specs|edge_case|comparison|use_case_specific|rebuscada",
  "info_necesaria": "qu√© info del JSON necesita o qu√© debe razonar"
}}"""

    try:
        response = openai.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=800,
            temperature=0.9  # M√°s creatividad para preguntas variadas
        )

        result_text = response.choices[0].message.content.strip()

        # Parsear JSON
        import re
        json_match = re.search(r'```json\s*(.*?)\s*```', result_text, re.DOTALL)
        if json_match:
            result_text = json_match.group(1)

        result = json.loads(result_text)

        # Mapear dificultad
        diff_map = {"facil": "easy", "media": "medium", "dificil": "hard"}
        difficulty = diff_map.get(result.get("dificultad", "media"), "medium")

        # Determinar expected behavior
        if difficulty == "easy":
            expected = "should_answer"
        elif difficulty == "medium":
            expected = "should_answer_or_low_confidence"
        else:
            expected = "should_handle_gracefully"

        # Convertir a formato de test
        question = {
            "text": result["pregunta"],
            "difficulty": difficulty,
            "type": result.get("tipo", "unknown"),
            "expected_behavior": expected,
            "info_needed": result.get("info_necesaria", "")
        }

        return [question]  # Retorna lista con 1 pregunta

    except Exception as e:
        print(f"    ‚ö†Ô∏è  Error generando preguntas IA: {e}")

        # Fallback: preguntas gen√©ricas
        return [
            {
                "text": "¬øDe qu√© color es?",
                "difficulty": "easy",
                "type": "simple",
                "expected_behavior": "should_answer"
            },
            {
                "text": "¬øCu√°nto dura la bater√≠a aproximadamente?",
                "difficulty": "medium",
                "type": "specs",
                "expected_behavior": "should_answer_or_low_confidence"
            },
            {
                "text": "¬øEs f√°cil de usar para alguien que no sabe mucho de tecnolog√≠a?",
                "difficulty": "hard",
                "type": "use_case_specific",
                "expected_behavior": "should_handle_gracefully"
            }
        ]


# ============================================================================
# TESTING
# ============================================================================

def test_product(product, questions):
    """Testea un producto con sus preguntas"""
    results = []

    for i, q in enumerate(questions, 1):
        difficulty_emoji = {"easy": "üü¢", "medium": "üü°", "hard": "üî¥"}
        emoji = difficulty_emoji.get(q["difficulty"], "‚ö™")

        print(f"    {emoji} [{i}/3] {q['difficulty'].upper()}: {q['text'][:60]}...")

        try:
            result_v2 = answer_question_v2(
                question=q["text"],
                asin=product["asin"],
                item_title=product["title"]
            )

            # Analizar resultado
            analysis = analyze_result(q, result_v2)

            results.append({
                "question": q,
                "result": result_v2,
                "analysis": analysis,
                "product_asin": product["asin"],
                "product_title": product["title"]
            })

            # Mostrar resultado breve
            status = "‚úÖ" if analysis["acceptable"] else "‚ùå"
            conf = result_v2.get('confidence', 0)
            action = result_v2.get('action', 'unknown')
            print(f"        {status} {action} (conf: {conf:.0f}%) - {analysis['verdict'][:50]}")

        except Exception as e:
            print(f"        ‚ùå ERROR: {str(e)[:60]}")
            results.append({
                "question": q,
                "result": {"error": str(e)},
                "analysis": {"acceptable": False, "verdict": f"Error: {e}"},
                "product_asin": product["asin"],
                "product_title": product["title"]
            })

    return results


def analyze_result(question, result):
    """Analiza si el resultado es aceptable"""
    expected = question.get("expected_behavior", "should_answer")
    action = result.get("action", "unknown")
    confidence = result.get("confidence", 0)
    notification_type = result.get("notification_type")

    analysis = {
        "acceptable": False,
        "verdict": "",
        "confidence": confidence,
        "action": action
    }

    # Criterios seg√∫n dificultad
    difficulty = question.get("difficulty", "medium")

    if difficulty == "easy":
        # F√°cil: DEBE responder con buena confidence
        if action == "answered" and confidence >= 70:
            analysis["acceptable"] = True
            analysis["verdict"] = f"‚úì Respondi√≥ bien (conf: {confidence:.0f}%)"
        elif action == "answered":
            analysis["acceptable"] = True
            analysis["verdict"] = f"‚ö†Ô∏è Respondi√≥ pero baja conf ({confidence:.0f}%)"
        else:
            analysis["acceptable"] = False
            analysis["verdict"] = f"‚úó No respondi√≥ pregunta f√°cil"

    elif difficulty == "medium":
        # Media: Puede responder o dar low_confidence
        if action == "answered":
            analysis["acceptable"] = True
            analysis["verdict"] = f"‚úì Respondi√≥ (conf: {confidence:.0f}%)"
        elif action == "no_answer" and result.get("reason") in ["low_confidence", "critical_question"]:
            analysis["acceptable"] = True
            analysis["verdict"] = f"‚úì Correctamente conservador ({result.get('reason')})"
        else:
            analysis["acceptable"] = False
            analysis["verdict"] = f"‚úó Acci√≥n inesperada: {action}"

    elif difficulty == "hard":
        # Dif√≠cil: Cualquier respuesta razonable est√° OK
        if action == "answered" and confidence >= 60:
            analysis["acceptable"] = True
            analysis["verdict"] = f"‚úì Respondi√≥ pregunta dif√≠cil (conf: {confidence:.0f}%)"
        elif action == "no_answer":
            analysis["acceptable"] = True
            analysis["verdict"] = f"‚úì Conservador en pregunta dif√≠cil"
        elif action == "answered":
            analysis["acceptable"] = True
            analysis["verdict"] = f"‚ö†Ô∏è Respondi√≥ con baja conf ({confidence:.0f}%)"
        else:
            analysis["acceptable"] = False
            analysis["verdict"] = f"? Comportamiento inesperado"

    return analysis


# ============================================================================
# REPORTE
# ============================================================================

def generate_report(all_results):
    """Genera reporte del mega test"""
    print("\n" + "="*80)
    print("üìä GENERANDO REPORTE DEL MEGA STRESS TEST")
    print("="*80)

    total = len(all_results)
    acceptable = sum(1 for r in all_results if r["analysis"]["acceptable"])

    # Por dificultad
    by_difficulty = {}
    for r in all_results:
        diff = r["question"].get("difficulty", "unknown")
        if diff not in by_difficulty:
            by_difficulty[diff] = {"total": 0, "acceptable": 0}
        by_difficulty[diff]["total"] += 1
        if r["analysis"]["acceptable"]:
            by_difficulty[diff]["acceptable"] += 1

    # Por tipo de pregunta
    by_type = {}
    for r in all_results:
        qtype = r["question"].get("type", "unknown")
        if qtype not in by_type:
            by_type[qtype] = {"total": 0, "acceptable": 0}
        by_type[qtype]["total"] += 1
        if r["analysis"]["acceptable"]:
            by_type[qtype]["acceptable"] += 1

    # Casos problem√°ticos
    problematic = [r for r in all_results if not r["analysis"]["acceptable"]]

    # Guardar reporte
    report = {
        "timestamp": datetime.now().isoformat(),
        "config": CONFIG,
        "summary": {
            "total_questions": total,
            "acceptable": acceptable,
            "problematic": total - acceptable,
            "acceptance_rate": (acceptable / total * 100) if total > 0 else 0
        },
        "by_difficulty": by_difficulty,
        "by_type": by_type,
        "problematic_cases": problematic[:20],  # Top 20
        "all_results": all_results
    }

    if CONFIG["save_results"]:
        output_dir = Path(CONFIG["output_dir"])
        output_dir.mkdir(exist_ok=True)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = output_dir / f"mega_stress_test_{timestamp}.json"

        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)

        print(f"\nüíæ Reporte guardado: {report_file}")

    # Mostrar resumen en consola
    print("\n" + "="*80)
    print("üìä RESUMEN DEL MEGA STRESS TEST")
    print("="*80)
    print(f"\nTotal de preguntas: {total}")
    print(f"‚úÖ Aceptables: {acceptable} ({acceptable/total*100:.1f}%)")
    print(f"‚ùå Problem√°ticas: {total - acceptable} ({(total-acceptable)/total*100:.1f}%)")

    print("\n" + "-"*80)
    print("üìà POR DIFICULTAD")
    print("-"*80)
    for diff in ["easy", "medium", "hard"]:
        if diff in by_difficulty:
            stats = by_difficulty[diff]
            rate = (stats["acceptable"] / stats["total"] * 100) if stats["total"] > 0 else 0
            emoji = "üü¢" if diff == "easy" else "üü°" if diff == "medium" else "üî¥"
            print(f"{emoji} {diff.upper():8} {stats['acceptable']:3}/{stats['total']:3} ({rate:5.1f}%)")

    print("\n" + "-"*80)
    print("üìä TOP TIPOS DE PREGUNTA")
    print("-"*80)
    sorted_types = sorted(by_type.items(), key=lambda x: x[1]["total"], reverse=True)
    for qtype, stats in sorted_types[:10]:
        rate = (stats["acceptable"] / stats["total"] * 100) if stats["total"] > 0 else 0
        print(f"{qtype:25} {stats['acceptable']:3}/{stats['total']:3} ({rate:5.1f}%)")

    if problematic:
        print("\n" + "-"*80)
        print("‚ö†Ô∏è  TOP 10 CASOS PROBLEM√ÅTICOS")
        print("-"*80)
        for i, case in enumerate(problematic[:10], 1):
            print(f"\n{i}. [{case['question']['difficulty'].upper()}] {case['product_title'][:50]}")
            print(f"   Pregunta: {case['question']['text'][:70]}")
            print(f"   Problema: {case['analysis']['verdict']}")

    print("\n" + "="*80)

    return report


# ============================================================================
# MAIN
# ============================================================================

def main():
    import sys
    # Force unbuffered output
    sys.stdout = os.fdopen(sys.stdout.fileno(), 'w', buffering=1)

    print("\n" + "#"*80, flush=True)
    print("# üöÄ MEGA STRESS TEST - Smart Answer Engine v2.0", flush=True)
    print("#"*80, flush=True)
    print(f"\nüìä Configuraci√≥n:", flush=True)
    print(f"  - Productos: {CONFIG['max_products']}", flush=True)
    print(f"  - Preguntas por producto: {CONFIG['questions_per_product']}", flush=True)
    print(f"  - Total esperado: ~{CONFIG['max_products'] * CONFIG['questions_per_product']} preguntas", flush=True)

    # 1. Cargar productos
    products = load_products_from_db(CONFIG["max_products"])

    if not products:
        print("\n‚ùå No se encontraron productos")
        return

    # 2. Generar y ejecutar tests
    all_results = []

    print(f"\nüß™ Iniciando tests con {len(products)} productos...")
    print("="*80)

    for i, product in enumerate(products, 1):
        print(f"\n[{i}/{len(products)}] üì¶ {product['title'][:60]}")
        print(f"  ASIN: {product['asin']} | Categor√≠a: {product['category'][:30]}")

        # Generar preguntas variadas
        questions = generate_realistic_questions(product)

        # Testear
        results = test_product(product, questions)
        all_results.extend(results)

        # Mostrar progreso cada 20
        if i % 20 == 0:
            current_acceptable = sum(1 for r in all_results if r["analysis"]["acceptable"])
            current_rate = (current_acceptable / len(all_results) * 100) if all_results else 0
            print(f"\n  üìä Progreso: {len(all_results)} preguntas, {current_acceptable} OK ({current_rate:.1f}%)")

    # 3. Generar reporte
    report = generate_report(all_results)

    print("\n‚úÖ Mega stress test completado")
    print(f"üìÅ Ver detalles en: {CONFIG['output_dir']}/")
    print()


if __name__ == "__main__":
    main()
