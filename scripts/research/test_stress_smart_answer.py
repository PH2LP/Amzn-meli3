#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Stress Test Exhaustivo para Smart Answer Engine v2.0

Genera preguntas dif√≠ciles con IA y prueba el sistema con productos reales.
Objetivo: Intentar que "pise el palito" y encontrar todos los casos extremos.
"""

import os
import sys
import json
import random
from pathlib import Path
from datetime import datetime
import openai
from dotenv import load_dotenv

sys.path.insert(0, 'scripts/tools')
from smart_answer_engine_v2 import answer_question_v2

load_dotenv()
openai.api_key = os.getenv("OPENAI_API_KEY")

# ============================================================================
# CONFIGURACI√ìN
# ============================================================================

CONFIG = {
    "max_products": 10,  # N√∫mero de productos a testear (m√°s r√°pido)
    "questions_per_product": 3,  # Preguntas por producto
    "question_difficulty": "high",  # low, medium, high, extreme
    "enable_comparison": True,  # Comparar con sistema viejo
    "save_results": True,
    "output_dir": "test_results_stress"
}

# ============================================================================
# CARGA DE PRODUCTOS
# ============================================================================

def get_available_products(max_products=100):
    """
    Obtiene lista de productos disponibles con mini_ml.
    """
    print(f"\nüì¶ Buscando productos disponibles...")

    products_dir = Path("storage/logs/publish_ready")
    mini_ml_files = list(products_dir.glob("*_mini_ml.json"))

    # Limitar cantidad
    mini_ml_files = mini_ml_files[:max_products]

    products = []

    for file_path in mini_ml_files:
        asin = file_path.stem.replace("_mini_ml", "")

        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)

            products.append({
                "asin": asin,
                "title": data.get("title_ai", "")[:80],
                "brand": data.get("brand", ""),
                "category": data.get("category_name", ""),
                "data": data
            })
        except Exception as e:
            print(f"  ‚ö†Ô∏è  Error cargando {asin}: {e}")

    print(f"  ‚úÖ {len(products)} productos cargados")
    return products


# ============================================================================
# GENERACI√ìN DE PREGUNTAS DIF√çCILES
# ============================================================================

QUESTION_TYPES = {
    "tricky_specifications": {
        "description": "Preguntas sobre specs t√©cnicas que pueden confundir",
        "examples": [
            "¬øEs compatible con Android 11 o necesita versi√≥n m√°s nueva?",
            "¬øLa bater√≠a es removible o viene integrada?",
            "¬øTiene puerto USB-C o es micro USB?",
            "¬øSoporta carga r√°pida de 65W o solo 45W?",
        ]
    },
    "comparison_questions": {
        "description": "Comparaciones dif√≠ciles entre versiones/modelos",
        "examples": [
            "¬øCu√°l es la diferencia entre este modelo y el Pro?",
            "¬øEs la versi√≥n 2023 o 2024?",
            "¬øEs el modelo internacional o nacional?",
        ]
    },
    "ambiguous_features": {
        "description": "Preguntas sobre features que pueden interpretarse mal",
        "examples": [
            "¬øTiene c√°mara? (en un videoportero vs smartphone)",
            "¬øUsa bater√≠as? (recargable vs desechables)",
            "¬øEs resistente al agua? (splash-proof vs waterproof)",
        ]
    },
    "quantity_and_contents": {
        "description": "Preguntas sobre cantidad y contenido del paquete",
        "examples": [
            "¬øCu√°ntos vienen en el paquete?",
            "¬øViene con cable de carga o hay que comprarlo aparte?",
            "¬øIncluye adaptador para 220v?",
        ]
    },
    "compatibility_questions": {
        "description": "Compatibilidad con otros dispositivos",
        "examples": [
            "¬øFunciona con iPhone 15?",
            "¬øEs compatible con Windows 11?",
            "¬øSirve para PS5 o solo PS4?",
        ]
    },
    "critical_safety": {
        "description": "Preguntas de seguridad que DEBEN notificar",
        "examples": [
            "¬øFunciona a 220v o necesita transformador?",
            "¬øTiene certificaci√≥n ANATEL?",
            "¬øEs seguro para ni√±os menores de 3 a√±os?",
        ]
    },
    "product_search": {
        "description": "B√∫squedas de otros productos (debe detectar)",
        "examples": [
            "¬øTen√©s el modelo XYZ disponible?",
            "¬øVend√©s tambi√©n auriculares Sony?",
            "¬øCu√°nto sale el iPhone 15 Pro?",
        ]
    },
    "multiple_questions": {
        "description": "Varias preguntas en una (debe manejar bien)",
        "examples": [
            "¬øDe qu√© color es y cu√°nto pesa?",
            "¬øViene con garant√≠a y se puede devolver?",
            "¬øEs original, tiene caja sellada y env√≠o gratis?",
        ]
    },
    "negatively_framed": {
        "description": "Preguntas formuladas negativamente",
        "examples": [
            "¬øNo usa pilas desechables, verdad?",
            "¬øNo es compatible con iOS?",
            "¬øNo necesita instalaci√≥n profesional?",
        ]
    },
    "edge_cases": {
        "description": "Casos extremos y confusos",
        "examples": [
            "Si lo uso 8 horas diarias, cu√°nto dura la bater√≠a?",
            "¬øEl color negro es mate o brillante?",
            "¬øLa garant√≠a cubre da√±os por agua si es resistente al agua?",
        ]
    }
}


def generate_difficult_questions(product_info, num_questions=5):
    """
    Genera preguntas dif√≠ciles y realistas usando IA.
    """

    # Preparar info del producto
    product_summary = f"""
PRODUCTO: {product_info['title']}
MARCA: {product_info['brand']}
CATEGOR√çA: {product_info['category']}
"""

    # Prompt para generar preguntas dif√≠ciles
    prompt = f"""Eres un cliente experto haciendo preguntas DIF√çCILES sobre productos en MercadoLibre.

{product_summary}

Genera {num_questions} preguntas REALISTAS pero DIF√çCILES que:
1. Puedan confundir a un sistema autom√°tico
2. Requieran entender el contexto del producto
3. Tengan trampas o ambig√ºedades
4. Sean del tipo que realmente hacen los clientes

Tipos de preguntas a incluir:
- Especificaciones t√©cnicas confusas
- Comparaciones con otros modelos
- Caracter√≠sticas ambiguas
- Cantidad y contenidos
- Compatibilidad
- Seguridad (alguna debe requerir notificaci√≥n)
- B√∫squeda de otros productos (1 para testear detecci√≥n)
- M√∫ltiples preguntas en una
- Formuladas negativamente

Responde SOLO este JSON:
{{
  "questions": [
    {{
      "text": "la pregunta en espa√±ol",
      "type": "tricky_specifications|comparison|ambiguous|quantity|compatibility|critical_safety|product_search|multiple|negative|edge_case",
      "difficulty": "medium|high|extreme",
      "trap": "descripci√≥n de la trampa o dificultad",
      "expected_behavior": "should_answer|should_notify|should_detect_search|should_ask_clarification"
    }},
    ...
  ]
}}

Importante:
- Al menos 1 pregunta tipo "critical_safety" (debe notificar)
- Al menos 1 pregunta tipo "product_search" (debe detectar)
- Al menos 2 preguntas muy dif√≠ciles (extreme difficulty)
- Preguntas variadas, no repetir el mismo patr√≥n"""

    try:
        response = openai.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=1500,
            temperature=0.8  # M√°s creatividad
        )

        result_text = response.choices[0].message.content.strip()

        # Parsear JSON
        import re
        json_match = re.search(r'```json\s*(.*?)\s*```', result_text, re.DOTALL)
        if json_match:
            result_text = json_match.group(1)

        result = json.loads(result_text)
        return result["questions"]

    except Exception as e:
        print(f"  ‚ö†Ô∏è  Error generando preguntas: {e}")

        # Fallback: preguntas gen√©ricas dif√≠ciles
        return [
            {
                "text": "¬øFunciona a 220v o necesita transformador?",
                "type": "critical_safety",
                "difficulty": "high",
                "trap": "Debe notificar por ser pregunta cr√≠tica el√©ctrica",
                "expected_behavior": "should_notify"
            },
            {
                "text": "¬øCu√°ntos vienen en el paquete?",
                "type": "quantity",
                "difficulty": "medium",
                "trap": "Puede confundir unidad con set",
                "expected_behavior": "should_answer"
            },
            {
                "text": "¬øTen√©s el modelo Pro disponible?",
                "type": "product_search",
                "difficulty": "medium",
                "trap": "Debe detectar b√∫squeda de otro producto",
                "expected_behavior": "should_detect_search"
            },
            {
                "text": "¬øDe qu√© color es y cu√°nto pesa?",
                "type": "multiple",
                "difficulty": "medium",
                "trap": "M√∫ltiples preguntas en una",
                "expected_behavior": "should_answer"
            },
            {
                "text": "¬øNo usa pilas desechables, verdad?",
                "type": "negative",
                "difficulty": "high",
                "trap": "Pregunta negativa que requiere respuesta positiva",
                "expected_behavior": "should_answer"
            }
        ]


# ============================================================================
# TESTING
# ============================================================================

def test_product_with_questions(product, questions):
    """
    Testea un producto con varias preguntas.
    """
    results = []

    for i, q in enumerate(questions, 1):
        print(f"\n  Pregunta {i}/{len(questions)}: {q['text'][:60]}...")

        # Ejecutar sistema v2
        try:
            result_v2 = answer_question_v2(
                question=q["text"],
                asin=product["asin"],
                item_title=product["title"]
            )

            # Analizar resultado
            analysis = analyze_result(q, result_v2)

            results.append({
                "question": q,
                "result_v2": result_v2,
                "analysis": analysis,
                "product_asin": product["asin"],
                "product_title": product["title"]
            })

            # Mostrar resultado breve
            status = "‚úÖ" if analysis["passed"] else "‚ùå"
            print(f"    {status} {analysis['verdict']}")

        except Exception as e:
            print(f"    ‚ùå ERROR: {e}")
            results.append({
                "question": q,
                "result_v2": {"error": str(e)},
                "analysis": {"passed": False, "verdict": f"Error: {e}"},
                "product_asin": product["asin"],
                "product_title": product["title"]
            })

    return results


def analyze_result(question, result_v2):
    """
    Analiza si el resultado es correcto seg√∫n lo esperado.
    """
    expected = question.get("expected_behavior", "should_answer")
    actual_action = result_v2.get("action", "unknown")
    notification_type = result_v2.get("notification_type")
    confidence = result_v2.get("confidence", 0)
    answer = result_v2.get("answer", "")

    analysis = {
        "passed": False,
        "verdict": "",
        "issues": [],
        "score": 0.0
    }

    # Verificar comportamiento esperado
    if expected == "should_notify":
        # Debe notificar (cr√≠tica o search)
        if result_v2.get("should_notify"):
            analysis["passed"] = True
            analysis["verdict"] = f"‚úì Correctamente notific√≥ ({notification_type})"
            analysis["score"] = 1.0
        else:
            analysis["passed"] = False
            analysis["verdict"] = "‚úó NO notific√≥ cuando deb√≠a"
            analysis["issues"].append("Deb√≠a notificar pero respondi√≥")
            analysis["score"] = 0.0

    elif expected == "should_detect_search":
        # Debe detectar b√∫squeda de producto
        if notification_type == "product_search":
            analysis["passed"] = True
            analysis["verdict"] = "‚úì Detect√≥ b√∫squeda de producto"
            analysis["score"] = 1.0
        else:
            analysis["passed"] = False
            analysis["verdict"] = "‚úó NO detect√≥ b√∫squeda de producto"
            analysis["issues"].append("Deb√≠a detectar product_search")
            analysis["score"] = 0.0

    elif expected == "should_answer":
        # Debe responder
        if actual_action == "answered":
            # Verificar calidad de respuesta
            issues_found = check_answer_quality(answer, question)

            if not issues_found:
                analysis["passed"] = True
                analysis["verdict"] = f"‚úì Respondi√≥ bien (conf: {confidence:.1f}%)"
                analysis["score"] = 1.0
            else:
                analysis["passed"] = False
                analysis["verdict"] = f"‚úó Respondi√≥ pero con problemas"
                analysis["issues"] = issues_found
                analysis["score"] = 0.5
        else:
            analysis["passed"] = False
            analysis["verdict"] = "‚úó NO respondi√≥ cuando deb√≠a"
            analysis["issues"].append(f"Acci√≥n: {actual_action}, Raz√≥n: {result_v2.get('reason')}")
            analysis["score"] = 0.0

    return analysis


def check_answer_quality(answer, question):
    """
    Verifica calidad de la respuesta.
    Detecta contradicciones, vaguedad, etc.
    """
    issues = []

    if not answer or len(answer) < 10:
        issues.append("Respuesta muy corta o vac√≠a")

    # Detectar contradicciones obvias
    answer_lower = answer.lower()

    contradiction_patterns = [
        (r"s√≠.*pero\s+no", "Contradicci√≥n: S√≠... pero no"),
        (r"no.*pero\s+s√≠", "Contradicci√≥n: No... pero s√≠"),
        (r"es.*no\s+es", "Contradicci√≥n: Es... no es"),
        (r"tiene.*no\s+tiene", "Contradicci√≥n: Tiene... no tiene"),
    ]

    import re
    for pattern, issue_desc in contradiction_patterns:
        if re.search(pattern, answer_lower):
            issues.append(issue_desc)

    # Detectar vaguedad
    vague_phrases = [
        "no tengo informaci√≥n",
        "consulta al vendedor",
        "verifica en la descripci√≥n",
        "no puedo confirmar",
        "[",  # Placeholders
        "..."
    ]

    for phrase in vague_phrases:
        if phrase in answer_lower:
            issues.append(f"Respuesta vaga: contiene '{phrase}'")

    return issues


# ============================================================================
# REPORTE
# ============================================================================

def generate_report(all_results):
    """
    Genera reporte completo de testing.
    """
    print("\n" + "="*80)
    print("üìä GENERANDO REPORTE COMPLETO")
    print("="*80)

    total_questions = len(all_results)
    passed = sum(1 for r in all_results if r["analysis"]["passed"])
    failed = total_questions - passed

    # Agrupar por tipo de error
    errors_by_type = {}
    for r in all_results:
        if not r["analysis"]["passed"]:
            issues = r["analysis"].get("issues", [])
            for issue in issues:
                errors_by_type[issue] = errors_by_type.get(issue, 0) + 1

    # Agrupar por tipo de pregunta
    by_question_type = {}
    for r in all_results:
        q_type = r["question"].get("type", "unknown")
        if q_type not in by_question_type:
            by_question_type[q_type] = {"total": 0, "passed": 0}
        by_question_type[q_type]["total"] += 1
        if r["analysis"]["passed"]:
            by_question_type[q_type]["passed"] += 1

    # Encontrar casos m√°s problem√°ticos
    worst_cases = sorted(
        [r for r in all_results if not r["analysis"]["passed"]],
        key=lambda x: x["analysis"]["score"]
    )[:10]

    # Generar reporte
    report = {
        "timestamp": datetime.now().isoformat(),
        "summary": {
            "total_questions": total_questions,
            "passed": passed,
            "failed": failed,
            "success_rate": (passed / total_questions * 100) if total_questions > 0 else 0
        },
        "by_question_type": by_question_type,
        "errors_by_type": errors_by_type,
        "worst_cases": worst_cases,
        "all_results": all_results
    }

    # Guardar JSON
    if CONFIG["save_results"]:
        output_dir = Path(CONFIG["output_dir"])
        output_dir.mkdir(exist_ok=True)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = output_dir / f"stress_test_{timestamp}.json"

        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)

        print(f"\nüíæ Reporte guardado: {report_file}")

    # Mostrar resumen en consola
    print("\n" + "="*80)
    print("üìä RESUMEN DEL STRESS TEST")
    print("="*80)
    print(f"\nTotal de preguntas testeadas: {total_questions}")
    print(f"‚úÖ Pasadas: {passed} ({passed/total_questions*100:.1f}%)")
    print(f"‚ùå Falladas: {failed} ({failed/total_questions*100:.1f}%)")

    print("\n" + "-"*80)
    print("üìà RENDIMIENTO POR TIPO DE PREGUNTA")
    print("-"*80)

    for q_type, stats in sorted(by_question_type.items(), key=lambda x: x[1]["passed"]/x[1]["total"] if x[1]["total"] > 0 else 0):
        total = stats["total"]
        passed_type = stats["passed"]
        rate = (passed_type / total * 100) if total > 0 else 0
        print(f"{q_type:25} {passed_type:3}/{total:3} ({rate:5.1f}%)")

    if errors_by_type:
        print("\n" + "-"*80)
        print("üêõ ERRORES M√ÅS COMUNES")
        print("-"*80)

        for error, count in sorted(errors_by_type.items(), key=lambda x: x[1], reverse=True)[:10]:
            print(f"{count:3}x  {error}")

    if worst_cases:
        print("\n" + "-"*80)
        print("‚ö†Ô∏è  TOP 10 CASOS M√ÅS PROBLEM√ÅTICOS")
        print("-"*80)

        for i, case in enumerate(worst_cases[:10], 1):
            print(f"\n{i}. ASIN: {case['product_asin']}")
            print(f"   Producto: {case['product_title'][:60]}")
            print(f"   Pregunta: {case['question']['text']}")
            print(f"   Tipo: {case['question']['type']} (dificultad: {case['question']['difficulty']})")
            print(f"   Problema: {case['analysis']['verdict']}")
            if case['analysis'].get('issues'):
                for issue in case['analysis']['issues']:
                    print(f"     - {issue}")

    print("\n" + "="*80)

    return report


# ============================================================================
# MAIN
# ============================================================================

def main():
    """
    Ejecuta stress test completo.
    """
    print("\n" + "#"*80)
    print("# STRESS TEST EXHAUSTIVO - SMART ANSWER ENGINE v2.0")
    print("#"*80)
    print(f"\nConfiguraci√≥n:")
    print(f"  - Productos a testear: {CONFIG['max_products']}")
    print(f"  - Preguntas por producto: {CONFIG['questions_per_product']}")
    print(f"  - Dificultad: {CONFIG['question_difficulty']}")

    # 1. Cargar productos
    products = get_available_products(CONFIG["max_products"])

    if not products:
        print("\n‚ùå No se encontraron productos para testear")
        return

    # 2. Generar y ejecutar tests
    all_results = []

    print(f"\nüß™ Iniciando tests con {len(products)} productos...")
    print("="*80)

    for i, product in enumerate(products, 1):
        print(f"\n[{i}/{len(products)}] Producto: {product['title']}")
        print(f"ASIN: {product['asin']}")

        # Generar preguntas dif√≠ciles
        questions = generate_difficult_questions(product, CONFIG["questions_per_product"])

        # Testear
        results = test_product_with_questions(product, questions)
        all_results.extend(results)

        # Mostrar progreso
        passed_count = sum(1 for r in results if r["analysis"]["passed"])
        print(f"  Resultado: {passed_count}/{len(results)} preguntas pasadas")

    # 3. Generar reporte
    report = generate_report(all_results)

    print("\n‚úÖ Stress test completado")
    print(f"üìÅ Ver detalles en: {CONFIG['output_dir']}/")
    print()


if __name__ == "__main__":
    main()
