#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Smart Answer Engine v2.0

Sistema inteligente de respuestas automÃ¡ticas usando:
- Chain-of-Thought reasoning
- Self-consistency validation
- Confidence scoring
- Smart notifications

Autor: Sistema mejorado - Diciembre 2024
"""

import os
import json
import re
from typing import Dict, List, Optional, Tuple
from datetime import datetime
from pathlib import Path
import openai
from dotenv import load_dotenv

load_dotenv()
openai.api_key = os.getenv("OPENAI_API_KEY")

# ============================================================================
# CONFIGURACIÃ“N
# ============================================================================

CONFIG = {
    "models": {
        "fast": "gpt-4o-mini",      # Para clasificaciÃ³n, extracciÃ³n, validaciÃ³n
        "smart": "gpt-4o",           # Para razonamiento principal
        "genius": "gpt-4o"           # Usar gpt-4o tambiÃ©n para complejos (o1-preview no disponible)
    },
    "confidence_thresholds": {
        "no_answer": 60,      # < 60% = No responder, notificar
        "review": 80,         # 60-80% = Responder pero notificar para revisiÃ³n
        "auto": 100           # > 80% = Responder automÃ¡tico
    },
    "enable_self_consistency": True,    # Usar self-consistency para crÃ­ticos
    "enable_validation": False,          # DESACTIVADO: causa falsos negativos
    "max_tokens_reasoning": 1000,
    "temperature_reasoning": 0.3,
    "logging_enabled": True
}

# ============================================================================
# SALUDO Y DESPEDIDA
# ============================================================================

def load_greeting_and_farewell():
    """Carga saludo y despedida del archivo docs/questions/saludo"""
    try:
        saludo_file = Path(__file__).parent.parent.parent / "docs" / "questions" / "saludo"

        with open(saludo_file, "r", encoding="utf-8") as f:
            content = f.read()

        # Parsear el archivo
        saludo = ""
        despedida = ""

        lines = content.split('\n')
        current_section = None

        for line in lines:
            if line.startswith("Saludo"):
                current_section = "saludo"
                continue
            elif line.startswith("Despedida"):
                current_section = "despedida"
                continue

            if current_section == "saludo" and line.strip():
                saludo = line.strip()
            elif current_section == "despedida" and line.strip():
                despedida = line.strip()

        return saludo, despedida

    except Exception as e:
        log(f"âš ï¸ Error cargando saludo/despedida: {e}")
        return "Â¡Hola! Gracias por tu consulta!", "Cualquier otra duda, estamos para ayudarte. Saludos!"

SALUDO, DESPEDIDA = load_greeting_and_farewell()

# ============================================================================
# UTILIDADES
# ============================================================================

def format_answer_with_greeting(answer: str) -> str:
    """
    Formatea la respuesta con saludo y despedida

    Args:
        answer: La respuesta generada por el sistema

    Returns:
        str: Saludo + respuesta + despedida
    """
    if not answer:
        return None

    return f"{SALUDO}\n\n{answer}\n\n{DESPEDIDA}"

def log(message: str, level: str = "INFO"):
    """Log con timestamp"""
    if CONFIG["logging_enabled"]:
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        print(f"[{timestamp}] [{level}] {message}")


def call_openai(
    prompt: str,
    model: str = "gpt-4o",
    max_tokens: int = 500,
    temperature: float = 0.3,
    response_format: Optional[dict] = None
) -> str:
    """
    Wrapper para llamadas a OpenAI con manejo de errores.
    """
    try:
        params = {
            "model": model,
            "messages": [{"role": "user", "content": prompt}],
            "max_tokens": max_tokens,
            "temperature": temperature
        }

        if response_format:
            params["response_format"] = response_format

        response = openai.chat.completions.create(**params)
        return response.choices[0].message.content.strip()

    except Exception as e:
        log(f"Error en llamada OpenAI: {e}", "ERROR")
        raise


def parse_json_response(text: str) -> dict:
    """
    Extrae JSON de una respuesta que puede tener texto adicional.
    """
    # Buscar bloques de cÃ³digo JSON
    json_pattern = r'```json\s*(.*?)\s*```'
    match = re.search(json_pattern, text, re.DOTALL)

    if match:
        json_str = match.group(1)
    else:
        # Intentar parsear directamente
        json_str = text

    try:
        return json.loads(json_str)
    except json.JSONDecodeError:
        # Buscar el primer { y el Ãºltimo }
        start = json_str.find('{')
        end = json_str.rfind('}') + 1
        if start != -1 and end > start:
            return json.loads(json_str[start:end])
        raise


# ============================================================================
# FASE 0: CLASIFICACIÃ“N INICIAL
# ============================================================================

def detect_product_search(question: str, item_title: Optional[str] = None) -> Dict:
    """
    Detecta si el cliente estÃ¡ buscando un producto especÃ­fico.
    Mejorado para reducir falsos positivos en comparaciones y compatibilidad.
    """
    log("Detectando bÃºsqueda de producto...")

    prompt = f"""Analiza si esta pregunta es una BÃšSQUEDA de producto DIFERENTE (el cliente quiere comprar OTRO producto).

PREGUNTA: "{question}"
PRODUCTO ACTUAL: {item_title or "N/A"}

REGLAS PARA CLASIFICAR:

âœ… SÃ ES BÃšSQUEDA - El cliente quiere COMPRAR otro producto diferente:
Indicadores clave: "tenÃ©s?", "vendÃ©s?", "disponible?", "en stock?", "cuÃ¡nto sale?"
Ejemplos:
- "TenÃ©s el iPhone 15 disponible?" â†’ BÃšSQUEDA (quiere comprar iPhone)
- "VendÃ©s tambiÃ©n auriculares Sony?" â†’ BÃšSQUEDA (quiere comprar auriculares)
- "Me interesa el modelo XYZ, lo tenÃ©s?" â†’ BÃšSQUEDA (quiere comprar XYZ)

âŒ NO ES BÃšSQUEDA - Pregunta sobre caracterÃ­sticas/compatibilidad de ESTE producto:

CASO 1 - COMPARACIÃ“N (menciona otro modelo para contexto):
- "Â¿CuÃ¡l es la diferencia con el modelo VT80H?" â†’ NO es bÃºsqueda (comparaciÃ³n)
- "Â¿Es mejor que el modelo anterior?" â†’ NO es bÃºsqueda (comparaciÃ³n genÃ©rica)
- "Si comparo con el A00114, Â¿este tiene mÃ¡s torque?" â†’ NO es bÃºsqueda (comparaciÃ³n)
Regla: Si usa verbos "comparar", "diferencia", "mejor que" â†’ NO es bÃºsqueda

CASO 2 - COMPATIBILIDAD (menciona otro producto para compatibilidad):
- "Â¿Es compatible con iPhone 15?" â†’ NO es bÃºsqueda (compatibilidad)
- "Â¿Funciona con el adaptador que comprÃ©?" â†’ NO es bÃºsqueda (compatibilidad)
- "Â¿Sirve para tornillos de la NASA?" â†’ NO es bÃºsqueda (compatibilidad hiperbÃ³lica)
- "Â¿Funciona con PS5?" â†’ NO es bÃºsqueda (compatibilidad)
Regla: Si pregunta si ESTE producto funciona/es compatible CON algo â†’ NO es bÃºsqueda

CASO 3 - PREGUNTA SOBRE ACCESORIOS/CONTENIDO:
- "Â¿Viene con cable USB-C?" â†’ NO es bÃºsqueda (quÃ© incluye)
- "Â¿Trae adaptador?" â†’ NO es bÃºsqueda (contenido)

PASO A PASO PARA CLASIFICAR:
1. Â¿Usa verbos de COMPRA/DISPONIBILIDAD? (tenÃ©s, vendÃ©s, disponible, stock) â†’ Probablemente SÃ bÃºsqueda
2. Â¿Usa verbos de COMPARACIÃ“N? (comparar, diferencia, mejor que) â†’ NO es bÃºsqueda
3. Â¿Usa verbos de COMPATIBILIDAD? (compatible, funciona con, sirve para) â†’ NO es bÃºsqueda
4. Â¿Pregunta sobre contenido/accesorios? (viene con, incluye, trae) â†’ NO es bÃºsqueda

CLAVE: Solo es bÃºsqueda si el cliente claramente quiere COMPRAR/ADQUIRIR un producto diferente.

Responde SOLO este JSON:
{{
  "is_product_search": true o false,
  "product_mentioned": "nombre del producto mencionado o null",
  "confidence": 0-100,
  "reasoning": "breve explicaciÃ³n - indica quÃ© caso es (comparaciÃ³n/compatibilidad/bÃºsqueda)"
}}"""

    response = call_openai(prompt, model=CONFIG["models"]["fast"], max_tokens=200)
    result = parse_json_response(response)

    log(f"BÃºsqueda detectada: {result['is_product_search']} (confidence: {result['confidence']}%)")
    return result


def detect_critical_question(question: str) -> Dict:
    """
    Detecta si es una pregunta tÃ©cnica crÃ­tica que requiere info precisa.
    Usa razonamiento IA para detectar conceptos de seguridad, no solo keywords.
    """
    log("Detectando pregunta crÃ­tica...")

    prompt = f"""Analiza si esta pregunta requiere informaciÃ³n PRECISA del fabricante porque involucra SEGURIDAD o LEGALIDAD.

PREGUNTA: "{question}"

Una pregunta es CRÃTICA si pregunta sobre:

ğŸ”´ SEGURIDAD ELÃ‰CTRICA (puede causar daÃ±os o incendios):
- Sobrecalentamiento, quemaduras, riesgo elÃ©ctrico que puede causar incendios
- Cortocircuitos o conexiones peligrosas
Ejemplos:
- "Â¿Incluye funciÃ³n que evite sobrecalentamiento?" â†’ CRÃTICA (seguridad tÃ©rmica)
- "Â¿Puede causar cortocircuito?" â†’ CRÃTICA
- "Â¿Se puede incendiar si lo uso mucho tiempo?" â†’ CRÃTICA

âŒ NO SON CRÃTICAS (info tÃ©cnica normal que estÃ¡ en specs):
- "Â¿Funciona a 220v?" â†’ NO crÃ­tica (es info de specs)
- "Â¿Funciona a 110v?" â†’ NO crÃ­tica (es info de specs)
- "Â¿Necesita transformador?" â†’ NO crÃ­tica (es info de specs)
- "Â¿QuÃ© voltaje usa?" â†’ NO crÃ­tica (es info de specs)

ğŸ”´ SEGURIDAD FÃSICA (puede causar lesiones):
- LÃ­mites de peso/carga que pueden causar colapso
- Resistencia estructural en condiciones extremas
- Seguridad en niÃ±os pequeÃ±os
Ejemplos:
- "Â¿Soporta cÃ¡mara de 10kg en exteriores ventosos?" â†’ CRÃTICA (lÃ­mite de carga + seguridad)
- "Â¿Es seguro para niÃ±os menores de 3 aÃ±os?"
- "Â¿El trÃ­pode aguanta eso sin caerse?" â†’ CRÃTICA (seguridad estructural)

ğŸ”´ SALUD:
- Alergias, toxicidad, materiales en contacto con piel/alimentos
Ejemplos:
- "Â¿Tiene materiales tÃ³xicos?"
- "Â¿Es hipoalergÃ©nico?"

ğŸ”´ LEGAL/CERTIFICACIONES:
- Certificaciones obligatorias (ANATEL, FCC, CE)
- GarantÃ­as oficiales
- Homologaciones
Ejemplos:
- "Â¿Tiene certificaciÃ³n ANATEL?"
- "Â¿Viene con garantÃ­a del fabricante?"

ğŸ”´ INFORMACIÃ“N DE CONTACTO (TOTALMENTE PROHIBIDO):
- NÃºmeros telefÃ³nicos, emails, direcciones, WhatsApp
- Cualquier dato de contacto del vendedor o fabricante
- UbicaciÃ³n fÃ­sica, domicilio, locales
Ejemplos:
- "Â¿CuÃ¡l es su nÃºmero de telÃ©fono?"
- "Â¿Tienen email de contacto?"
- "Â¿DÃ³nde estÃ¡n ubicados?"
- "Â¿Me pasÃ¡s tu WhatsApp?"
- "Â¿CuÃ¡l es la direcciÃ³n de su local?"

âŒ NO SON CRÃTICAS (preguntas normales sobre caracterÃ­sticas):
- "Â¿De quÃ© color es?"
- "Â¿CuÃ¡nto pesa?"
- "Â¿Es compatible con iPhone?" (compatibilidad funcional, no elÃ©ctrica)
- "Â¿Viene con cable USB?" (accesorios incluidos)
- "Â¿La baterÃ­a dura mucho?" (duraciÃ³n, no seguridad)
- "Â¿Es resistente al agua?" (caracterÃ­stica del producto)
- "Is it water-resistant?" (caracterÃ­stica del producto)
- "Â¿QuÃ© funciones tiene?" (funcionalidades)
- "What functions does it have?" (funcionalidades)
- "Â¿Tiene GPS?" (caracterÃ­stica)
- "Â¿Funciona con Bluetooth?" (conectividad)

IMPORTANTE:
- Si la pregunta menciona CONSECUENCIAS NEGATIVAS (daÃ±os, roturas, quemaduras, colapso) â†’ Probablemente ES crÃ­tica
- Si solo pregunta sobre CARACTERÃSTICAS del producto (resistencia al agua, funciones, GPS, etc.) â†’ NO es crÃ­tica

Responde SOLO este JSON:
{{
  "is_critical": true o false,
  "category": "electrical_safety|physical_safety|health_safety|legal|contact_info|none",
  "safety_concern": "descripciÃ³n del riesgo de seguridad detectado",
  "confidence": 0-100,
  "reasoning": "por quÃ© es o no es crÃ­tica"
}}"""

    try:
        response = call_openai(prompt, model=CONFIG["models"]["fast"], max_tokens=300, temperature=0.1)
        result = parse_json_response(response)

        if result.get("is_critical"):
            log(f"âš ï¸  Pregunta CRÃTICA detectada: {result.get('category')} (confidence: {result.get('confidence')}%)")
            log(f"    Riesgo: {result.get('safety_concern', 'N/A')}")
            return {
                "is_critical": True,
                "category": result.get("category", "unknown"),
                "keyword_matched": result.get("safety_concern", ""),
                "reason": result.get("reasoning", "Pregunta crÃ­tica de seguridad detectada")
            }
        else:
            log(f"Pregunta NO crÃ­tica (confidence: {result.get('confidence')}%)")
            return {"is_critical": False}

    except Exception as e:
        log(f"Error en detecciÃ³n de crÃ­ticas: {e}", "WARNING")
        # Fallback a keywords bÃ¡sicos
        critical_keywords = ["voltaje", "voltage", "110v", "220v", "transformador",
                           "certificaciÃ³n", "anatel", "garantÃ­a", "tÃ³xico", "alergia"]
        question_lower = question.lower()
        for keyword in critical_keywords:
            if keyword in question_lower:
                log(f"âš ï¸  Fallback: keyword '{keyword}' detectada como crÃ­tica")
                return {
                    "is_critical": True,
                    "category": "fallback_detection",
                    "keyword_matched": keyword,
                    "reason": f"Keyword crÃ­tica detectada: {keyword}"
                }
        return {"is_critical": False}


# ============================================================================
# FASE 1: EXTRACCIÃ“N DE INFORMACIÃ“N DEL PRODUCTO
# ============================================================================

def load_product_data(asin: str) -> Tuple[Optional[dict], Optional[dict]]:
    """
    Carga datos del producto desde mini_ml y/o amazon_json.
    Si no existen, intenta descargarlos con SP API.
    """
    mini_ml = None
    amazon_json = None

    # Intentar cargar mini_ml
    mini_ml_path = f"storage/logs/publish_ready/{asin}_mini_ml.json"
    if os.path.exists(mini_ml_path):
        try:
            with open(mini_ml_path, 'r', encoding='utf-8') as f:
                mini_ml = json.load(f)
            log(f"mini_ml cargado: {mini_ml_path}")
        except Exception as e:
            log(f"Error cargando mini_ml: {e}", "WARNING")

    # Intentar cargar amazon_json
    amazon_json_path = f"storage/asins_json/{asin}.json"
    if os.path.exists(amazon_json_path):
        try:
            with open(amazon_json_path, 'r', encoding='utf-8') as f:
                amazon_json = json.load(f)
            log(f"amazon_json cargado: {amazon_json_path}")
        except Exception as e:
            log(f"Error cargando amazon_json: {e}", "WARNING")

    # Si no hay datos, intentar descargar con SP API
    if not mini_ml and not amazon_json:
        log(f"No se encontraron datos locales para {asin}, descargando con SP API...")
        try:
            import sys
            sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../..')))
            from src.integrations.amazon_api import get_product_data_from_asin

            log(f"Descargando producto {asin} desde Amazon SP API...")
            # Guardar directamente en storage/asins_json/
            os.makedirs("storage/asins_json", exist_ok=True)
            product_data = get_product_data_from_asin(asin, save_path=amazon_json_path)

            if product_data and os.path.exists(amazon_json_path):
                log(f"âœ… Producto {asin} descargado y guardado en {amazon_json_path}")
                amazon_json = product_data
            else:
                log(f"âš ï¸ No se pudo descargar informaciÃ³n para {asin}", "WARNING")
        except Exception as e:
            log(f"Error descargando producto con SP API: {e}", "ERROR")

    return mini_ml, amazon_json


def extract_smart_context(asin: str) -> Dict:
    """
    Extrae informaciÃ³n estructurada del producto usando IA.

    Retorna un contexto rico y relevante, no solo copia de datos.
    """
    log(f"Extrayendo contexto inteligente para ASIN {asin}...")

    # 1. Cargar datos disponibles
    mini_ml, amazon_json = load_product_data(asin)

    if not mini_ml and not amazon_json:
        log("No hay datos del producto disponibles", "ERROR")
        return {
            "error": "no_data",
            "completeness_score": 0.0
        }

    # 2. Preparar datos para anÃ¡lisis
    # Usar mini_ml preferentemente (mÃ¡s compacto y limpio)
    source_data = mini_ml or amazon_json

    # Limitar tamaÃ±o para el prompt (12000 chars para capturar mÃ¡s info)
    source_json_str = json.dumps(source_data, ensure_ascii=False)[:12000]

    # 3. Prompt para extracciÃ³n inteligente
    extraction_prompt = f"""Analiza este producto y extrae TODA la informaciÃ³n tÃ©cnica disponible.

DATOS DEL PRODUCTO (JSON completo):
{source_json_str}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
INSTRUCCIONES DE EXTRACCIÃ“N DINÃMICA
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1. BUSCA informaciÃ³n en TODOS estos campos del JSON:
   - title / itemName / title_ai
   - bullet_points / bullet_point / features
   - attributes / attributes_mapped / main_characteristics / second_characteristics
   - description / description_ai
   - whats_included / package_includes
   - dimensions / weight / package
   - battery / power / voltage
   - connectivity / wireless / bluetooth / wifi
   - memory / storage / flash_memory
   - warranty / guarantee
   - material / color / style
   - Â¡Y CUALQUIER OTRO CAMPO QUE ENCUENTRES!

2. EXTRAE TODAS LAS ESPECIFICACIONES TÃ‰CNICAS que encuentres:
   - NO te limites a categorÃ­as predefinidas
   - Si ves un campo con informaciÃ³n Ãºtil â†’ INCLÃšYELO en key_specs
   - Ejemplos de lo que debes buscar:
     * Almacenamiento: "flash_memory", "storage", "SD", "microSD", "128GB"
     * BaterÃ­a: "battery", "rechargeable", "40H", "AAA", "lithium"
     * Dimensiones: "dimensions", "weight", "size", "height", "width", "depth"
     * Conectividad: "bluetooth", "wifi", "USB", "Lightning", "wireless"
     * Resistencia: "waterproof", "IP65", "water_resistant"
     * Material: "plastic", "metal", "aluminum", "stainless steel"
     * Voltaje: "110v", "220v", "voltage", "power_source"
     * Profundidad de teclas: "key_travel", "key_depth", "stroke"
     * Peso mÃ¡ximo: "max_weight", "capacity", "load_capacity"
     * Cualquier spec tÃ©cnica: sensor, resolution, speed, rpm, watts, etc.

3. FORMATO para key_specs:
   - Usa nombres descriptivos en espaÃ±ol
   - Valor claro y conciso
   - Si NO encuentras una spec â†’ NO la incluyas (no pongas "no especificada")
   - Ejemplo: "almacenamiento": "Tarjetas Micro SD (no incluidas)"
   - Ejemplo: "profundidad_teclas": "4mm de recorrido"
   - Ejemplo: "peso_maximo": "Soporta hasta 150kg"

4. Extrae las 10 caracterÃ­sticas MÃS IMPORTANTES para un comprador

5. Da un score de completitud (0.0-1.0) segÃºn cuÃ¡nta info Ãºtil encontraste

Responde SOLO este JSON:
{{
  "product_type": "tipo especÃ­fico de producto",
  "purpose": "para quÃ© sirve en 1 lÃ­nea",
  "brand": "marca",
  "title_short": "tÃ­tulo resumido en 60 caracteres",
  "top_features": ["feature 1", "feature 2", ..., "feature 10"],
  "key_specs": {{
    "spec_name_1": "valor claro y Ãºtil",
    "spec_name_2": "valor claro y Ãºtil",
    ...
    (INCLUYE TODAS las specs tÃ©cnicas que encuentres - puede ser 5, 10, 20 o mÃ¡s)
  }},
  "whats_included": ["item 1", "item 2", ...],
  "completeness_score": 0.0-1.0
}}

IMPORTANTE:
- Extrae TODO lo que encuentres - no te limites
- USA nombres descriptivos en espaÃ±ol para las specs
- Si algo estÃ¡ en inglÃ©s en el JSON, tradÃºcelo al espaÃ±ol
- NO inventes informaciÃ³n que no estÃ¡ en el JSON"""

    try:
        response = call_openai(
            extraction_prompt,
            model=CONFIG["models"]["fast"],
            max_tokens=3000,  # Tokens generosos para extraer todas las specs posibles
            temperature=0.1
        )

        context = parse_json_response(response)
        log(f"Contexto extraÃ­do (completeness: {context.get('completeness_score', 0):.2f})")

        return context

    except Exception as e:
        log(f"Error extrayendo contexto: {e}", "ERROR")

        # Fallback: usar datos bÃ¡sicos sin IA
        fallback_context = {
            "product_type": "unknown",
            "purpose": source_data.get("title_ai", source_data.get("title", ""))[:100],
            "brand": source_data.get("brand", ""),
            "completeness_score": 0.3
        }
        return fallback_context


# ============================================================================
# FASE 2: RAZONAMIENTO CON CHAIN-OF-THOUGHT
# ============================================================================

def generate_answer_with_reasoning(
    question: str,
    product_context: Dict,
    use_genius_model: bool = False
) -> Dict:
    """
    Genera respuesta usando Chain-of-Thought reasoning.
    """
    log("Generando respuesta con razonamiento estructurado...")

    # Seleccionar modelo
    model = CONFIG["models"]["genius"] if use_genius_model else CONFIG["models"]["smart"]

    # Preparar contexto del producto en formato legible
    context_str = json.dumps(product_context, ensure_ascii=False, indent=2)

    # Prompt con Chain-of-Thought
    reasoning_prompt = f"""Eres un VENDEDOR de MercadoLibre respondiendo preguntas de clientes en tu publicaciÃ³n.

CONTEXTO IMPORTANTE:
- RespondÃ©s DIRECTO como vendedor, sin formalidades excesivas
- Si no tenÃ©s la informaciÃ³n especÃ­fica, NO INVENTÃ‰S ni des respuestas genÃ©ricas
- Para temas de IMPORTACIÃ“N/ADUANAS/IMPUESTOS: ExplicÃ¡ que MercadoLibre se encarga de toda la gestiÃ³n

INFORMACIÃ“N DEL PRODUCTO:
{context_str}

PREGUNTA DEL CLIENTE:
"{question}"

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

PIENSA PASO A PASO:

<thinking>
PASO 1 - ENTENDER EL PRODUCTO:
- Â¿QuÃ© tipo de producto es exactamente?
- Â¿Para quÃ© se usa?
- Â¿CuÃ¡les son sus 3 caracterÃ­sticas principales?

PASO 2 - ANALIZAR EL TIPO DE PREGUNTA:
- Â¿Es una pregunta SIMPLE (color, peso, tamaÃ±o)?
- Â¿Es COMPARACIÃ“N con otro modelo? â†’ EnfÃ³cate en ESTE producto, no puedes comparar
- Â¿Son MÃšLTIPLES preguntas en una? â†’ Identifica TODAS y responde TODAS
- Â¿EstÃ¡ formulada NEGATIVAMENTE? â†’ Cuidado con doble negaciÃ³n ("Â¿No usa pilas, verdad?" â†’ si NO usa = confirmar que es recargable)
- Â¿Pregunta COMPATIBILIDAD? â†’ Busca specs tÃ©cnicas (conectores, voltaje, sistemas)

PASO 3 - ENTENDER LA INTENCIÃ“N:
- Â¿QuÃ© quiere saber REALMENTE el cliente?
- Â¿Por quÃ© pregunta esto? Â¿QuÃ© le preocupa?
- Â¿QuÃ© informaciÃ³n le ayudarÃ­a a tomar la decisiÃ³n de compra?

PASO 4 - BUSCAR LA INFORMACIÃ“N:
- La INFORMACIÃ“N DEL PRODUCTO tiene todas las specs extraÃ­das del JSON
- Busca en key_specs la informaciÃ³n que el cliente necesita
- Ejemplo: si pregunta "memory" o "storage", busca en specs: "almacenamiento", "storage", "memoria"
- Ejemplo: si pregunta "garantÃ­a", busca: "garantia", "warranty"
- Ejemplo: si pregunta "peso", busca: "peso", "weight"
- Ejemplo: si pregunta "profundidad de teclas", busca: "profundidad_teclas", "key_travel", "recorrido"
- Â¿Tengo la informaciÃ³n especÃ­fica que necesita en las specs?
- Si son mÃºltiples preguntas, identifica cuÃ¡les SÃ puedes responder
- IMPORTANTE: Si son varias preguntas y no puedes responder TODAS, responde SOLO las que SÃ sabes
- NO menciones que falta informaciÃ³n o que no sabes algo

PASO 5 - EVALUAR CONFIANZA:
- Â¿Puedo responder con CERTEZA basÃ¡ndome en los datos?
- Â¿O la informaciÃ³n es insuficiente/ambigua?
- Si faltan datos para alguna parte, Â¿quÃ© confidence dar?
- Nivel de confianza: ____%

PASO 6 - PLANEAR LA RESPUESTA:
- Â¿CÃ³mo presentar la info de forma clara y Ãºtil?
- Si son mÃºltiples preguntas, estructurar respuesta punto por punto
- Si es comparaciÃ³n, explicar que solo puedo hablar de ESTE producto
- Si es negativa, asegurarme de responder correctamente la negaciÃ³n
- Â¿QuÃ© tono usar? (amigable, tÃ©cnico, persuasivo)
- Â¿Hay algo POSITIVO que destacar?
</thinking>

AHORA GENERA TU RESPUESTA EN ESTE FORMATO:

INSTRUCCIONES PARA CADA TAG:
- <answer> = Tu respuesta de 2-5 lÃ­neas, tono amigable, SIN saludos ni despedidas
- <confidence> = Solo el nÃºmero de 0 a 100
- <key_points> = Puntos clave que usaste para responder

FORMATO (escribe TODO entre los tags, NO dejes tags vacÃ­os):

<answer></answer>

<confidence></confidence>

<key_points></key_points>

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

REGLAS ABSOLUTAS:

1. NUNCA inventes informaciÃ³n que no estÃ¡ en el contexto

2. âš ï¸ MUY IMPORTANTE - FALTA DE INFORMACIÃ“N:
   Si NO tienes la informaciÃ³n especÃ­fica que el cliente pregunta:
   - PON confidence=0 (CERO)
   - DEJA answer VACÃO o con texto mÃ­nimo explicando que no tienes esa info
   - Ejemplos de preguntas SIN INFO: watts, amperes, potencia, consumo elÃ©ctrico especÃ­fico
   - Si el JSON no tiene esa spec tÃ©cnica â†’ confidence=0 SIEMPRE
   - NO des respuestas genÃ©ricas tipo "Te recomendarÃ­a verificar con el fabricante"

3. ğŸš¢ IMPORTACIÃ“N, ADUANAS E IMPUESTOS - Regla especial:
   Si preguntan sobre:
   - Costos de importaciÃ³n
   - Aranceles o tarifas aduaneras
   - Impuestos de importaciÃ³n
   - GestiÃ³n de aduana
   - TrÃ¡mites de importaciÃ³n

   RESPUESTA EXACTA:
   "MercadoLibre se encarga de toda la gestiÃ³n de importaciÃ³n, aduanas e impuestos. El precio publicado es el precio final que pagÃ¡s, sin costos adicionales sorpresa."

   Confidence: 95% (es informaciÃ³n general de cÃ³mo funciona ML)
   NUNCA digas "verificÃ¡ con el vendedor" o "consultar con ML" - vos SOS el vendedor

4. SÃ© POSITIVO pero HONESTO - destaca beneficios sin mentir

5. Evita lenguaje robÃ³tico o frases template

6. NO generes saludos ("Hola") ni despedidas - se agregan automÃ¡ticamente

7. Empieza DIRECTO con la respuesta

8. Si preguntan "funciona con X" y tiene "Y integrado", destaca lo POSITIVO de Y primero

9. NUNCA te contradigas en la misma respuesta

10. Si es COMPARACIÃ“N, enfÃ³cate solo en ESTE producto

11. Si es NEGATIVA, entiende bien: "Â¿No usa pilas?" = pregunta si NO usa pilas

12. VOLTAJE - Reglas importantes:
    - Si el JSON dice "110-120V" â†’ Menciona "clavija americana recta" + "necesitarÃ¡ transformador para 220V"
    - Si el JSON NO especifica voltaje â†’ Di "Su ficha tÃ©cnica no indica el voltaje especÃ­fico, usualmente este tipo de dispositivos funcionan tanto con 110V como 220V"
    - NUNCA digas "no se especifica" o "te recomendarÃ­a verificar" - suena poco profesional

EJEMPLO VOLTAJE CON INFO EN JSON:
Pregunta: "Â¿Funciona a 220v?"
Contexto: voltage: "110-120V"
Respuesta: "El producto funciona con 110-120V y clavija americana recta, por lo que si tienes 220V en tu paÃ­s necesitarÃ¡s un transformador de voltaje para su correcto funcionamiento."

EJEMPLO VOLTAJE SIN INFO EN JSON:
Pregunta: "Â¿Funciona a 220v?"
Contexto: voltage: null o no mencionado
Respuesta EXACTA: "Su ficha tÃ©cnica no indica el voltaje especÃ­fico, usualmente este tipo de dispositivos funcionan tanto con 110v como 220v"

IMPORTANTE: Usa EXACTAMENTE esta frase cuando no haya voltaje en JSON
"""

    try:
        response = call_openai(
            reasoning_prompt,
            model=model,
            max_tokens=CONFIG["max_tokens_reasoning"],
            temperature=CONFIG["temperature_reasoning"]
        )

        # Parsear respuesta estructurada
        result = parse_structured_response(response)

        log(f"Respuesta generada (confidence: {result['confidence']}%)")
        return result

    except Exception as e:
        log(f"Error generando respuesta: {e}", "ERROR")
        return {
            "answer": "",
            "confidence": 0,
            "thinking": "",
            "key_points": [],
            "error": str(e)
        }


def parse_structured_response(response_text: str) -> Dict:
    """
    Parsea respuesta estructurada con tags.
    """
    result = {
        "thinking": "",
        "answer": "",
        "confidence": 0,
        "key_points": []
    }

    # Extraer <thinking>
    thinking_match = re.search(r'<thinking>(.*?)</thinking>', response_text, re.DOTALL)
    if thinking_match:
        result["thinking"] = thinking_match.group(1).strip()

    # Extraer <answer>
    answer_match = re.search(r'<answer>(.*?)</answer>', response_text, re.DOTALL)
    if answer_match:
        result["answer"] = answer_match.group(1).strip()

    # Extraer <confidence>
    confidence_match = re.search(r'<confidence>(.*?)</confidence>', response_text, re.DOTALL)
    if confidence_match:
        confidence_text = confidence_match.group(1).strip()
        # Extraer nÃºmero
        numbers = re.findall(r'\d+', confidence_text)
        if numbers:
            result["confidence"] = int(numbers[0])

    # Extraer <key_points>
    keypoints_match = re.search(r'<key_points>(.*?)</key_points>', response_text, re.DOTALL)
    if keypoints_match:
        keypoints_text = keypoints_match.group(1).strip()
        # Convertir a lista si es posible
        result["key_points"] = keypoints_text

    return result


# ============================================================================
# FASE 3: VALIDACIÃ“N Y CONFIDENCE SCORING
# ============================================================================

def calculate_final_confidence(
    result: Dict,
    product_context: Dict,
    question: str
) -> Dict:
    """
    Calcula confidence final considerando mÃºltiples factores.
    """
    log("Calculando confidence final...")

    factors = []

    # Factor 1: Confidence del modelo (peso 55% - aumentado para compensar)
    model_confidence = result.get("confidence", 0)
    factors.append(("model_confidence", model_confidence, 0.55))

    # Factor 2: Completitud de informaciÃ³n del producto (peso 20%)
    info_completeness = product_context.get("completeness_score", 0.5) * 100
    factors.append(("info_completeness", info_completeness, 0.2))

    # Factor 3: Longitud de respuesta apropiada (peso 10%)
    answer_length = len(result.get("answer", "").split())
    # 15-50 palabras es ideal
    if 15 <= answer_length <= 50:
        length_score = 100
    elif answer_length < 15:
        length_score = max(0, (answer_length / 15) * 100)
    else:  # > 50
        length_score = max(50, 100 - ((answer_length - 50) * 2))
    factors.append(("answer_length", length_score, 0.1))

    # Factor 4: No contiene palabras sospechosas (peso 15% - reducido)
    # Palabras que indican incertidumbre REAL o falta de informaciÃ³n
    critical_uncertain_words = [
        "no tengo informaciÃ³n",
        "no sÃ©",
        "no estoy seguro",
        "no puedo confirmar",
        "debÃ©s consultar",
        "no especifica",  # Movido a crÃ­ticas - indica falta de info
        "no indica",      # Similar a "no especifica"
        "no menciona",    # Similar a "no especifica"
        "no se especifica",
        "no estÃ¡ especificado",
        "[", "]"  # Placeholders
    ]

    # Palabras menos crÃ­ticas (pueden ser vÃ¡lidas en ciertos contextos)
    minor_uncertain_words = [
        "consulta",
        "verifica"
    ]

    answer_lower = result.get("answer", "").lower()

    # Contar palabras crÃ­ticas vs menores
    critical_count = sum(1 for word in critical_uncertain_words if word in answer_lower)
    minor_count = sum(1 for word in minor_uncertain_words if word in answer_lower)

    # PenalizaciÃ³n gradual en vez de binaria
    if critical_count > 0:
        suspicious_score = 0  # Muy sospechoso
    elif minor_count > 1:
        suspicious_score = 50  # Algo sospechoso
    elif minor_count == 1:
        suspicious_score = 80  # Ligeramente sospechoso
    else:
        suspicious_score = 100  # Sin problemas

    factors.append(("no_suspicious_words", suspicious_score, 0.15))  # Peso reducido de 20% a 15%

    # Calcular weighted average
    final_confidence = sum(score * weight for _, score, weight in factors)

    log(f"Confidence final: {final_confidence:.1f}%")
    log(f"  - Model: {model_confidence}%")
    log(f"  - Info completeness: {info_completeness:.1f}%")
    log(f"  - Answer length: {length_score:.1f}%")
    log(f"  - No suspicious: {suspicious_score}%")

    return {
        "final_confidence": final_confidence,
        "factors": factors
    }


def check_contradictions(answer: str, product_context: Dict) -> Dict:
    """
    Verifica que la respuesta no tenga contradicciones.
    """
    if not CONFIG["enable_validation"]:
        return {"has_contradictions": False, "score": 100}

    log("Validando coherencia...")

    product_type = product_context.get("product_type", "producto")

    validation_prompt = f"""Verifica esta respuesta de un vendedor:

TIPO DE PRODUCTO: {product_type}
RESPUESTA: "{answer}"

Verifica:
1. Â¿Tiene contradicciones internas? (ej: "SÃ­... pero no es")
2. Â¿Es coherente con el tipo de producto?
3. Â¿El tono es apropiado para ventas? (amigable, Ãºtil, no robÃ³tico)

Responde SOLO este JSON:
{{
  "has_contradictions": true/false,
  "issues": ["problema 1", "problema 2", ...],
  "is_coherent": true/false,
  "tone_appropriate": true/false,
  "score": 0-100
}}"""

    try:
        response = call_openai(
            validation_prompt,
            model=CONFIG["models"]["fast"],
            max_tokens=300,
            temperature=0.1
        )

        validation = parse_json_response(response)

        if validation.get("has_contradictions"):
            log(f"âš ï¸  Contradicciones detectadas: {validation.get('issues')}", "WARNING")

        return validation

    except Exception as e:
        log(f"Error en validaciÃ³n: {e}", "WARNING")
        return {"has_contradictions": False, "score": 100}


def apply_self_consistency(
    question: str,
    product_context: Dict,
    n: int = 3
) -> Dict:
    """
    Genera N respuestas y selecciona la mÃ¡s consistente.
    Solo para preguntas crÃ­ticas.
    """
    if not CONFIG["enable_self_consistency"]:
        return generate_answer_with_reasoning(question, product_context)

    log(f"Aplicando self-consistency con {n} muestras...")

    responses = []

    # Generar N respuestas con temperatura ligeramente diferente
    for i in range(n):
        temp = 0.2 + (i * 0.15)  # 0.2, 0.35, 0.5

        result = generate_answer_with_reasoning(question, product_context)
        result["temperature"] = temp
        responses.append(result)

    # Usar GPT para seleccionar la mejor
    answers_text = "\n\n".join([
        f"RESPUESTA {i+1} (confidence {r['confidence']}%):\n{r['answer']}"
        for i, r in enumerate(responses)
    ])

    selection_prompt = f"""Analiza estas {n} respuestas a la misma pregunta sobre un producto:

PREGUNTA: "{question}"

{answers_text}

Â¿CuÃ¡l es la MÃS PRECISA, ÃšTIL y CONSISTENTE?

Responde SOLO este JSON:
{{
  "best_index": 0 o 1 o 2,
  "reason": "breve explicaciÃ³n de 1 lÃ­nea"
}}"""

    try:
        response = call_openai(
            selection_prompt,
            model=CONFIG["models"]["fast"],
            max_tokens=200
        )

        selection = parse_json_response(response)
        best_index = selection.get("best_index", 0)

        log(f"Self-consistency seleccionÃ³ respuesta #{best_index + 1}: {selection.get('reason')}")

        best_response = responses[best_index]
        best_response["self_consistency_applied"] = True

        return best_response

    except Exception as e:
        log(f"Error en self-consistency, usando primera respuesta: {e}", "WARNING")
        return responses[0]


# ============================================================================
# SISTEMA PRINCIPAL
# ============================================================================

def answer_question_v2(
    question: str,
    asin: str,
    item_title: Optional[str] = None,
    question_id: Optional[int] = None,
    customer_nickname: Optional[str] = None,
    site_id: Optional[str] = None
) -> Dict:
    """
    Sistema completo de respuestas v2.0

    Retorna:
    {
        "action": "answered" | "no_answer",
        "answer": str | None,
        "confidence": float,
        "reason": str,
        "should_notify": bool,
        "notification_type": str,
        "metadata": {...}
    }
    """
    log(f"\n{'='*80}")
    log(f"Procesando pregunta: {question[:60]}...")
    log(f"ASIN: {asin}")

    result = {
        "action": None,
        "answer": None,
        "confidence": 0.0,
        "reason": "",
        "should_notify": False,
        "notification_type": None,
        "metadata": {}
    }

    try:
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # FASE 0: CLASIFICACIÃ“N INICIAL
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

        # Detectar bÃºsqueda de producto
        search_detection = detect_product_search(question, item_title)

        if search_detection["is_product_search"] and search_detection["confidence"] > 75:
            log("âœ‹ BÃºsqueda de producto detectada - NO responder")
            result.update({
                "action": "no_answer",
                "reason": "product_search",
                "should_notify": True,
                "notification_type": "product_search",
                "metadata": {
                    "product_searched": search_detection.get("product_mentioned"),
                    "detection_confidence": search_detection["confidence"]
                }
            })
            return result

        # Detectar pregunta crÃ­tica
        critical = detect_critical_question(question)

        if critical["is_critical"]:
            log(f"âš ï¸  Pregunta crÃ­tica detectada ({critical['category']}) - NO responder")
            result.update({
                "action": "no_answer",
                "reason": "critical_question",
                "should_notify": True,
                "notification_type": "critical_question",
                "metadata": {
                    "critical_category": critical["category"],
                    "keyword_matched": critical.get("keyword_matched")
                }
            })
            return result

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # FASE 1: EXTRACCIÃ“N DE INFORMACIÃ“N
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

        product_context = extract_smart_context(asin)

        if product_context.get("error") == "no_data":
            log("âŒ No hay datos del producto - NO responder")
            result.update({
                "action": "no_answer",
                "reason": "no_product_data",
                "should_notify": True,
                "notification_type": "no_data"
            })
            return result

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # FASE 2: GENERACIÃ“N CON RAZONAMIENTO
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

        # Decidir si usar modelo genius para preguntas complejas
        use_genius = (
            critical.get("is_critical", False) or
            len(question.split()) > 20 or  # Pregunta muy larga
            "?" in question and question.count("?") > 1  # MÃºltiples preguntas
        )

        if use_genius:
            log(f"ğŸ§  Usando modelo genius ({CONFIG['models']['genius']}) para pregunta compleja")

        # Generar respuesta
        answer_result = generate_answer_with_reasoning(
            question,
            product_context,
            use_genius_model=use_genius
        )

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # FASE 3: VALIDACIÃ“N Y CONFIDENCE
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

        # Calcular confidence final
        confidence_analysis = calculate_final_confidence(
            answer_result,
            product_context,
            question
        )
        final_confidence = confidence_analysis["final_confidence"]

        # Validar coherencia si confidence es suficiente
        if final_confidence > 40:
            validation = check_contradictions(
                answer_result["answer"],
                product_context
            )

            if not validation.get("is_coherent", True):
                log("âš ï¸  Respuesta incoherente detectada - penalizando levemente")
                final_confidence = final_confidence * 0.9

            if validation.get("has_contradictions", False):
                log("âš ï¸  Contradicciones detectadas - penalizando confidence")
                final_confidence = final_confidence * 0.6

        # Self-consistency para preguntas crÃ­ticas con low confidence
        if (critical.get("is_critical", False) and
            final_confidence < 90 and
            final_confidence > 50):
            log("ğŸ”„ Aplicando self-consistency para mejorar confidence...")
            answer_result = apply_self_consistency(question, product_context, n=3)
            # Recalcular confidence
            confidence_analysis = calculate_final_confidence(
                answer_result,
                product_context,
                question
            )
            final_confidence = min(95, confidence_analysis["final_confidence"] + 5)

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # DECISIÃ“N FINAL
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

        thresholds = CONFIG["confidence_thresholds"]

        if final_confidence < thresholds["no_answer"]:
            # NO responder - confidence muy bajo
            log(f"âŒ Confidence muy bajo ({final_confidence:.1f}%) - NO responder")
            result.update({
                "action": "no_answer",
                "reason": "low_confidence",
                "confidence": final_confidence,
                "should_notify": True,
                "notification_type": "low_confidence",
                "metadata": {
                    "generated_answer": answer_result["answer"],
                    "reasoning": answer_result.get("thinking", "")[:300],
                    "confidence_factors": confidence_analysis["factors"]
                }
            })

        elif final_confidence < thresholds["review"]:
            # Responder PERO notificar para revisiÃ³n
            log(f"âš ï¸  Confidence medio ({final_confidence:.1f}%) - Responder + notificar")
            result.update({
                "action": "answered",
                "answer": format_answer_with_greeting(answer_result["answer"]),
                "confidence": final_confidence,
                "reason": "medium_confidence",
                "should_notify": True,
                "notification_type": "review_recommended",
                "metadata": {
                    "reasoning": answer_result.get("thinking", "")[:300],
                    "confidence_factors": confidence_analysis["factors"]
                }
            })

        else:
            # Responder con confianza - NO notificar
            log(f"âœ… Confidence alto ({final_confidence:.1f}%) - Responder automÃ¡tico")
            result.update({
                "action": "answered",
                "answer": format_answer_with_greeting(answer_result["answer"]),
                "confidence": final_confidence,
                "reason": "high_confidence",
                "should_notify": False,
                "metadata": {
                    "reasoning": answer_result.get("thinking", "")[:200]
                }
            })

        log(f"{'='*80}\n")
        return result

    except Exception as e:
        log(f"âŒ Error inesperado: {e}", "ERROR")
        import traceback
        traceback.print_exc()

        result.update({
            "action": "no_answer",
            "reason": "system_error",
            "should_notify": True,
            "notification_type": "error",
            "metadata": {"error": str(e)}
        })
        return result


# ============================================================================
# TESTING
# ============================================================================

if __name__ == "__main__":
    # Test bÃ¡sico
    print("\n" + "="*80)
    print("SMART ANSWER ENGINE v2.0 - TEST")
    print("="*80 + "\n")

    # Caso de prueba
    test_question = "Does it work at 220v or does it need a transformer?"
    test_asin = "B0BFJWCYTL"  # Reemplazar con un ASIN real

    result = answer_question_v2(
        question=test_question,
        asin=test_asin,
        item_title="Test Product"
    )

    print("\n" + "="*80)
    print("RESULTADO:")
    print("="*80)
    print(json.dumps(result, indent=2, ensure_ascii=False))
    print()
