#!/usr/bin/env python3
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 21_extract_keywords.py - EXTRAER KEYWORDS DE CATEGORÃAS ML
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#
# Â¿QuÃ© hace?
#   Lee el archivo html_categories.txt con categorÃ­as de MercadoLibre y genera keywords de Amazon.
#   Usa OpenAI API para analizar el texto y mantener contexto jerÃ¡rquico.
#
#   IMPORTANTE: Mantiene el contexto de categorÃ­a padre + subcategorÃ­a
#   Ejemplo: Si ve "BebÃ©s" â†’ "Juguetes", genera "baby toys" (NO solo "toys")
#
# Input:
#   - html_categories.txt
#
# Output:
#   - keywords_from_ml_categories.txt (keywords Ãºnicos, ordenados alfabÃ©ticamente)
#
# Comando:
#   python3 21_extract_keywords.py
#
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
import os
import json
from openai import OpenAI
from pathlib import Path
from dotenv import load_dotenv

load_dotenv(override=True)

# Inicializar cliente OpenAI
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

def extract_categories_from_chunk(text_chunk, chunk_num, total_chunks):
    """Extrae categorÃ­as de un chunk de texto usando GPT-4"""
    print(f"ğŸ“ Procesando chunk {chunk_num}/{total_chunks}...")

    # Llamar a OpenAI API
    try:
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {
                    "role": "user",
                    "content": f"""Eres un experto en e-commerce que convierte categorÃ­as trending de MercadoLibre en keywords ESPECÃFICAS y COMPLETAS para buscar productos en Amazon.

TEXTO A ANALIZAR (Chunk {chunk_num}/{total_chunks}):
{text_chunk}

CONTEXTO IMPORTANTE:
Estas son categorÃ­as TRENDING de MercadoLibre. Tu trabajo es convertirlas en keywords ESPECÃFICAS que se puedan usar para BUSCAR productos similares en Amazon.

REGLA CRÃTICA #1: NUNCA generes keywords genÃ©ricas de 1 palabra
âŒ MAL: "reptiles", "toys", "grooming", "food"
âœ… BIEN: "reptile terrariums", "reptile tanks", "cat toys", "dog grooming brushes"

REGLA CRÃTICA #2: SIEMPRE incluye el contexto completo
- Si dice "Other categories of Reptiles & Amphibians" y luego "Terrariums, Tanks & Boxes"
  â†’ Genera: "reptile terrariums", "reptile tanks", "reptile enclosures", "amphibian terrariums"

REGLA CRÃTICA #3: Piensa como un COMPRADOR de Amazon
- Â¿QuÃ© buscarÃ­a alguien que quiere comprar esto?
- Usa tÃ©rminos completos y especÃ­ficos que se usan en Amazon
- Incluye variaciones comunes

EJEMPLOS REALES:

Input: "Other categories of Animals and Pets" â†’ "Pet Treats"
Output:
dog treats
cat treats
pet training treats
healthy pet treats
natural pet treats
dental dog treats

Input: "Other categories of Reptiles & Amphibians" â†’ "Terrariums, Tanks & Boxes"
Output:
reptile terrariums
reptile tanks
reptile enclosures
glass terrariums
amphibian tanks
snake terrariums
lizard tanks

Input: "Other categories of Cats" â†’ "Grooming & Care"
Output:
cat grooming brushes
cat nail clippers
cat grooming gloves
cat deshedding tools
cat grooming kit
cat dental care

Input: "Other categories of Horses" â†’ "Grooming & Care"
Output:
horse grooming brushes
horse curry combs
horse hoof picks
horse grooming kit
horse mane brush
horse tail brush

REGLAS DE EXPANSIÃ“N:
1. Si ves "Toys" bajo "Cats", genera mÃºltiples keywords: "cat toys", "interactive cat toys", "cat toy balls", "cat feather toys", etc.
2. Si ves "Food", expande: "dog food", "cat food", "dry dog food", "wet cat food", "organic pet food"
3. Piensa en VARIACIONES que un comprador real buscarÃ­a
4. Incluye ADJETIVOS comunes: "natural", "organic", "premium", "training", "interactive", "automatic"
5. Incluye TIPOS especÃ­ficos cuando sea relevante

FILTROS ESTRICTOS:
- IGNORA "Other" como categorÃ­a
- IGNORA porcentajes y nÃºmeros
- IGNORA "Sales in this category..."
- NO generes keywords de 1 sola palabra
- NO generes keywords genÃ©ricos sin contexto

FORMATO DE SALIDA:
- Un keyword por lÃ­nea
- Sin numeraciÃ³n, sin viÃ±etas
- Solo el keyword en inglÃ©s (minÃºsculas)
- MÃ­nimo 2 palabras por keyword
- MÃ¡ximo 4-5 palabras por keyword

IMPORTANTE: Genera entre 3-8 variaciones de keywords por cada categorÃ­a, pensando en cÃ³mo buscarÃ­a un comprador REAL en Amazon."""
                }
            ],
            max_tokens=4000
        )

        # Extraer keywords de la respuesta
        keywords_text = response.choices[0].message.content

        # Limpiar keywords
        keywords = []
        for line in keywords_text.split('\n'):
            k = line.strip().lower()

            # Filtrar lÃ­neas vacÃ­as, comentarios, y respuestas de error de GPT
            if not k or k.startswith('#') or k.startswith('âŒ') or k.startswith('âœ…'):
                continue

            # Filtrar respuestas de error comunes de GPT
            if "sorry" in k or "can't assist" in k or "cannot" in k:
                continue

            # Filtrar keywords que terminen en "other"
            if k.endswith(" other") or k == "other":
                continue

            # Filtrar lÃ­neas que contengan caracteres especiales de formato
            if '**' in k or 'â†’' in k or k.endswith(':'):
                continue

            # Filtrar si contiene "input:" o "output:" (ejemplos de GPT)
            if 'input:' in k or 'output:' in k:
                continue

            # Remover texto innecesario
            k = k.replace(" in baby safety", "")
            k = k.replace(" in baby", "")

            # Filtrar si tiene menos de 2 palabras
            if len(k.split()) < 2:
                continue

            # Agregar si no quedÃ³ vacÃ­o
            if k:
                keywords.append(k)

        # Extraer usage de tokens
        usage = response.usage
        input_tokens = usage.prompt_tokens
        output_tokens = usage.completion_tokens
        total_tokens = usage.total_tokens

        # Calcular costo (GPT-4o pricing: $2.50 per 1M input tokens, $10.00 per 1M output tokens)
        cost_input = (input_tokens / 1_000_000) * 2.50
        cost_output = (output_tokens / 1_000_000) * 10.00
        total_cost = cost_input + cost_output

        print(f"   âœ… ExtraÃ­das {len(keywords)} keywords")
        print(f"   ğŸ“Š Tokens: {total_tokens:,} (input: {input_tokens:,}, output: {output_tokens:,})")
        print(f"   ğŸ’µ Costo: ${total_cost:.4f}")

        return keywords, {"input_tokens": input_tokens, "output_tokens": output_tokens, "total_tokens": total_tokens, "cost": total_cost}

    except Exception as e:
        print(f"   âŒ Error: {e}")
        return [], {"input_tokens": 0, "output_tokens": 0, "total_tokens": 0, "cost": 0.0}

def main():
    print("=" * 80)
    print("EXTRACCIÃ“N DE KEYWORDS DESDE html_categories.txt")
    print("=" * 80)
    print()

    # Leer archivo html_categories.txt
    input_file = Path(__file__).parent / "html_categories.txt"

    if not input_file.exists():
        print(f"âŒ No se encontrÃ³ el archivo: {input_file}")
        return

    with open(input_file, 'r', encoding='utf-8') as f:
        lines = f.readlines()

    total_lines = len(lines)
    print(f"ğŸ“‚ Archivo encontrado: {input_file}")
    print(f"ğŸ“„ Total lÃ­neas: {total_lines:,}")
    print()

    # Dividir en chunks de 200 lÃ­neas para no exceder lÃ­mites de tokens
    LINES_PER_CHUNK = 200
    chunks = []

    for i in range(0, total_lines, LINES_PER_CHUNK):
        chunk = ''.join(lines[i:i + LINES_PER_CHUNK])
        chunks.append(chunk)

    total_chunks = len(chunks)
    print(f"ğŸ“¦ Dividido en {total_chunks} chunks de ~{LINES_PER_CHUNK} lÃ­neas cada uno")
    print()

    # Procesar cada chunk
    all_keywords = []
    total_stats = {
        "input_tokens": 0,
        "output_tokens": 0,
        "total_tokens": 0,
        "cost": 0.0
    }

    for idx, chunk in enumerate(chunks, 1):
        keywords, stats = extract_categories_from_chunk(chunk, idx, total_chunks)
        all_keywords.extend(keywords)

        # Acumular estadÃ­sticas
        total_stats["input_tokens"] += stats["input_tokens"]
        total_stats["output_tokens"] += stats["output_tokens"]
        total_stats["total_tokens"] += stats["total_tokens"]
        total_stats["cost"] += stats["cost"]

        print()

    # Cargar keywords existentes si el archivo ya existe
    output_file = Path(__file__).parent / "keywords_from_ml_categories.txt"
    existing_keywords = set()

    if output_file.exists():
        with open(output_file, 'r', encoding='utf-8') as f:
            existing_keywords = set(line.strip() for line in f if line.strip())
        print(f"ğŸ“ Cargadas {len(existing_keywords)} keywords existentes del archivo")

    # Combinar keywords nuevas con existentes
    all_keywords_set = set(all_keywords)
    all_keywords_set.update(existing_keywords)

    # Ordenar alfabÃ©ticamente
    sorted_keywords = sorted(all_keywords_set, key=str.lower)

    # Guardar en archivo (sobrescribir con lista completa)
    with open(output_file, 'w', encoding='utf-8') as f:
        for keyword in sorted_keywords:
            f.write(f"{keyword}\n")

    new_keywords_count = len(all_keywords_set) - len(existing_keywords)

    # Resumen
    print("=" * 80)
    print("âœ… PROCESAMIENTO COMPLETADO")
    print("=" * 80)
    print(f"Keywords extraÃ­das:      {len(all_keywords)}")
    print(f"Keywords existentes:     {len(existing_keywords)}")
    print(f"Keywords nuevas:         {new_keywords_count}")
    print(f"Total keywords Ãºnicas:   {len(sorted_keywords)}")
    print(f"Archivo guardado:        {output_file}")
    print()
    print("ğŸ“Š CONSUMO TOTAL:")
    print(f"   Tokens totales:       {total_stats['total_tokens']:,}")
    print(f"   - Input tokens:       {total_stats['input_tokens']:,}")
    print(f"   - Output tokens:      {total_stats['output_tokens']:,}")
    print(f"   ğŸ’µ Costo total:       ${total_stats['cost']:.4f}")
    print()

    # Mostrar preview
    print("ğŸ“‹ Preview de keywords (primeras 20):")
    for i, keyword in enumerate(sorted_keywords[:20], 1):
        print(f"   {i:2d}. {keyword}")

    if len(sorted_keywords) > 20:
        print(f"   ... y {len(sorted_keywords) - 20} mÃ¡s")
    print()

if __name__ == "__main__":
    main()
